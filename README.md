# Depth Estimation from Stereo Images

![Depth Estimation Example](depth_estimation.png)

This project implements depth estimation and 3D point cloud visualization from stereo images using the KITTI dataset, with both traditional computer vision approaches and advanced deep learning methods.

## Features

- Compute depth maps from stereo image pairs
- Generate colored 3D point clouds from depth maps
- Visualize point clouds in the camera coordinate frame
- Deep learning-based stereo depth estimation with transformer architectures

## Approaches

### Traditional Computer Vision (baseline.py)
- Uses OpenCV's StereoSGBM algorithm for stereo matching
- Computes disparity maps and converts to depth
- Creates and visualizes 3D point clouds

### Deep Learning (ml_depth/)
- DINOv2-based stereo transformer network with LoRA adapters
- Cross-attention mechanism for stereo feature fusion
- Self-attention refinement for high-quality disparity maps

## Requirements

All requirements can be installed using:
```bash
pip install -r requirements.txt
```

## Installation

```bash
# Clone the repository
git clone https://github.com/mudit1729/depth_estimation.git
cd depth_estimation

# Install dependencies
pip install -r requirements.txt
```

## Usage

### Traditional Approach

Run the baseline script with stereo image pairs:

```bash
python baseline.py --left path/to/left/image.png --right path/to/right/image.png
```

Parameters:
- `--left`: Path to the left image (from image_2 directory)
- `--right`: Path to the right image (from image_3 directory)
- `--focal`: Focal length in pixels (default KITTI: 721.5377)
- `--baseline`: Baseline in meters (default KITTI: 0.54)
- `--cx`: Principal point x-coordinate (default KITTI: 609.5593)
- `--cy`: Principal point y-coordinate (default KITTI: 172.854)
- `--max_depth`: Maximum depth to visualize (default: 100 meters)

### Deep Learning Approach

Train the model:
```bash
python ml_depth/train.py --data_dir /path/to/kitti --batch_size 4 --num_epochs 20
```

Monitor training with TensorBoard:
```bash
tensorboard --logdir=runs
```

Evaluate the model:
```bash
python ml_depth/evaluate.py --data_dir /path/to/kitti --model_path checkpoints/best_model.pth
```

## Model Architecture

The deep learning model consists of:

1. **Dual DINOv2-small Backbone**: Two vision transformers process the left and right stereo images separately
2. **LoRA Adapters**: Low-rank adaptation for efficient fine-tuning of the foundation models
3. **Cross-Attention Fusion**: Attention mechanism to correlate features between left and right views
4. **Self-Attention Refinement**: Two self-attention layers to refine the spatial features
5. **Disparity Head**: Final regression to disparity values

The model outputs a disparity map that can be converted to depth using the formula:
```
depth = (focal_length * baseline) / disparity
```

## Evaluation Metrics

Performance is measured using industry-standard stereo vision metrics:

- **D1**: Percentage of disparity outliers (>3px error or >5% of ground truth)
- **D2**: Percentage of disparity outliers with stricter threshold (>2px or >3%)
- **RMSE**: Root Mean Square Error for disparity and depth
- **Abs Rel**: Absolute Relative Error
- **Î´ < 1.25**: Percentage of predictions within 25% of ground truth

All metrics are tracked during training and visualized in TensorBoard.

## Dataset

This project uses the KITTI Stereo 2015 dataset. The dataset is not included in this repository and can be downloaded from the [KITTI website](http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php).

## License

MIT